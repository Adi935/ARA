# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xvErRDY_gOWf7YtEEELXEv-Dft67V_wS
"""

import numpy as np
#class to initialize all the parameters, no of layers and units in each layer
class initialization:
    parameters={}
    activation_funcs={}
    Z={}
    A={}
#Constructor to initialize the layers,learning rate,the input vector and labels    
    def __init__(self,X,Y,layers,learning_rate):
        self.L=layers
        self.Y=Y
        self.X=X
        self.m=Y.shape[1]
        self.learning_rate=learning_rate
#Function to initialize the parameters for each layer         
    def initialize_units(self):
        output =self.Y
        X=self.X
        layers = [X.shape[0]]   
        for l in range (1,self.L+1) :
           x=int(input(f"Enter no of units in layer{l}"))
           layers.append(x)
        for l in range (1,self.L+1):
            self.parameters[f"W{l}"] = np.random.randn(layers[l-1],layers[l])*np.sqrt(2/layers[l-1])
            self.parameters[f"b{l}"] = np.zeros((layers[l],output.shape[1]),dtype=float)
        self.parameters[f"W{self.L+1}"]=np.random.randn(layers[self.L],output.shape[0])
        self.parameters[f"b{self.L+1}"]=np.zeros((output.shape[0],output.shape[1]),dtype=float)
        return self.parameters  
#Function to initialize activation functions for each layer          
    def activation_func(self):
        for i in range(1,self.L+1):
            self.activation_funcs[f"act{i}"]= input(f"Enter the activation for layer{i}")
        self.activation_funcs[f"act{self.L+1}"]    =input("Last layer activation")
        return self.activation_funcs
# This class defines all the activation functions
class activation : 
    def sigmoid (self,Z):
        return 1/(1+np.exp(-Z))
    def Relu(self,Z):
        Z=np.maximum(Z,0)
        return Z
#Class for forward propogation        
class forwardprop (initialization,activation):
    def _init_(self):  #Constructor that initializes Ycap to a numpy array of zeros and the cost to 0 
        Y=self.Y
        self.Ycap = np.zeros((Y.shape[0],Y.shape[1]),dtype=float)
        self.cost=0
    def linear (self,A_prev,W,b):  #Function that calculates the linear part 
        Z= np.dot(W.T,A_prev) +b
        return Z
    def forward(self):       #Function for the entire forward propogation 
        self.A[f"A{0}"] = self.X
        A_prev=self.X 
        for l in range (1,self.L+1) :
            self.Z[f"Z{l}"]=self.linear(A_prev,self.parameters[f"W{l}"],self.parameters[f"b{l}"])
            if(self.activation_funcs[f"act{l}"]=="sigmoid"):
                self.A[f"A{l}"] = self.sigmoid(self.Z[f"Z{l}"])
            else :
                self.A[f"A{l}"] = self.Relu(self.Z[f"Z{l}"])
            A_prev=  self.A[f"A{l}"]   

        self.Z[f"Z{self.L+1}"]=self.linear(A_prev,self.parameters[f"W{self.L+1}"],self.parameters[f"b{self.L+1}"])  
        self.A[f"A{self.L+1}"] = self.sigmoid(self.Z[f"Z{self.L+1}"])
        self.Ycap=self.A[f"A{self.L+1}"]
    def Costcal(self):    #Function that calculates the cost 
        self.cost =-1/self.m*np.sum(self.Y*np.log(self.Ycap)+(1-self.Y)*np.log(1-self.Ycap))
#Class for derivatives of activation functions   
class Derivativefunc(forwardprop):

    def sigmoidback(self):     #Derivative of cost wrt to Sigmoid
        Ycap=self.Ycap
        return np.multiply(Ycap,(1-Ycap))
    def REluback(self,A):      #Derivative of Cost wrt to Relu function
        A[A<=0]=0
        A[A>0]=1
        return A
#Class for backpropogation        
class Backprop(Derivativefunc,forwardprop) :
     grads={}
     def linear_back(self,dZ,A_prev,W):    #function that calculates dW,db,dA_prev
        dW=1/self.m*np.dot(dZ,A_prev.T)
        db=1/self.m*np.sum(dZ,axis=1,keepdims=True)
        dA_prev=np.dot(W,dZ)
        return dW,db,dA_prev
     def actiback(self,dA,A,activation):  #Function that calculates dZ
        if activation =="relu" :
            der=self.REluback(A)
            dZ=np.multiply(dA,der)
        elif activation =="sigmoid" :
            der=self.sigmoidback()
            dZ=np.multiply(dA,der)
        return dZ
     def getgrads(self):               #Function that calculates grads for all the parameters in each layer 
         self.grads[f"dA{self.L+1}"] = - (np.divide(self.Y,self.A[f"A{self.L+1}"]) - np.divide(1 - self.Y, 1 - self.A[f"A{self.L+1}"]))
         dA=self.grads[f"dA{self.L+1}"]
         A=self.A[f"A{self.L+1}"]
         W=self.parameters[f"W{self.L+1}"]
         b=self.parameters[f"b{self.L+1}"]
         self.grads[f"dZ{self.L+1}"] = self.actiback(dA,A,self.activation_funcs[f"act{self.L+1}"])
         dZ=self.grads[f"dZ{self.L+1}"]
         A_prev=self.A[f"A{(self.L)}"]
         self.grads[f"dW{self.L+1}"],self.grads[f"db{self.L+1}"],self.grads[f"dA{(self.L)}"]=self.linear_back(dZ,A_prev,W)
         for l in range(self.L,0,-1) :
            A=self.A[f"A{l}"]
            W=self.parameters[f"W{l}"]
            self.grads[f"dZ{l}"] = self.actiback(self.grads[f"dA{l}"],A,self.activation_funcs[f"act{l}"])
            dZ=self.grads[f"dZ{l}"]
            self.grads[f"dW{l}"],self.grads[f"db{l}"],self.grads[f"dA{l-1}"]=self.linear_back(dZ,self.A[f"A{l-1}"],W)
         return self.grads
#Class for training the parameters         
class training (Backprop) :
    def updation(self):    #Function to update the parameters using SGD optimization Algorithm 
        L=self.L
        learning_rate=self.learning_rate
        for l in range (L+1) :
            self.parameters[f"W{l+1}"] = self.parameters[f"W{l+1}"] - learning_rate*self.grads[f"dW{l+1}"].T
            self.parameters[f"b{l+1}"] = self.parameters[f"b{l+1}"] - learning_rate*self.grads[f"db{l+1}"]